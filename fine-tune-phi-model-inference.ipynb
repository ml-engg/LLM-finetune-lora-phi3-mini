{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./phi-3-mini-custom\"  # Path to your fine-tuned model directory\n",
    "model_infer = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True,\\\n",
    "                                                   device_map = 'cuda')\n",
    "tokenizer_infer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Add special tokens if needed\n",
    "special_tokens = {'additional_special_tokens': ['<|instruction|>', '<|input|>', '<|output|>', '<|end|>']}\n",
    "tokenizer_infer.add_special_tokens(special_tokens)\n",
    "model_infer.resize_token_embeddings(len(tokenizer_infer))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_infer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_infer.to(device)\n",
    "\n",
    "def apply_peft_chat_template_infer(instruction, input_text):\n",
    "    formatted_prompt = (\n",
    "        f\"<|instruction|> {instruction} <|end|>\\n\"\n",
    "        f\"<|input|> {input_text} <|end|>\\n\"\n",
    "        f\"<|output|>\"\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "# Function to perform inference for a single prompt\n",
    "def generate_response(instruction, input_text, max_length=100, temperature=0.7, top_p=0.9):\n",
    "    # Format the prompt\n",
    "    prompt = apply_peft_chat_template_infer(instruction, input_text)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    #input_ids = tokenizer_infer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = tokenizer_infer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Generate the output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model_infer.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the output part\n",
    "    output_start = generated_text.find(\"<|output|>\") + len(\"<|output|>\")\n",
    "    output_end = generated_text.find(\"<|end|>\", output_start)\n",
    "    response = generated_text[output_start:output_end].strip() if output_end != -1 else generated_text[output_start:].strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./phi-3-mini-custom\")\n",
    "tokenizer.save_pretrained(\"./phi-3-mini-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Example usage\n",
    "instruction = \"Generate a python function to print intergers.\"\n",
    "input_text = \"The function should take a number and print till that.\"\n",
    "\n",
    "# Generate the response\n",
    "response = generate_response(instruction, input_text)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
